{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "## For Binary Classification\n",
    "\n",
    "### Decision Function\n",
    "\n",
    "The decision function for a Support Vector Machine (SVM) in binary classification is:\n",
    "\n",
    "\n",
    "### W<sup>T</sup> X + b = Label\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( W \\) is the weight vector.\n",
    "- \\( X \\) is the input vector.\n",
    "- \\( b \\) is the bias term.\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "This equation represents a hyperplane that separates the two classes in feature space.\n",
    "\n",
    "### Classification:\n",
    "\n",
    "1. **Positive Class:** If \\( W^T X + b > 0 \\), the input \\( X \\) is classified as belonging to one class.\n",
    "2. **Negative Class:** If \\( W^T X + b < 0 \\), the input \\( X \\) is classified as belonging to the other class.\n",
    "\n",
    "### Objective of SVM\n",
    "\n",
    "The objective of SVM is to find the hyperplane that maximizes the margin between the two classes. This is done by placing support vectors (key data points) on the margin boundaries to improve classification accuracy and robustness.\n",
    "\n",
    "\n",
    "### SVM Components\n",
    "\n",
    "<ol>\n",
    "<li>HyperPlane</li><br>\n",
    "\n",
    "<li>Support Vector</li><br>\n",
    "\n",
    "<li>Margin</li><br>\n",
    "    \n",
    "<li>Kernels</li><br>\n",
    "\n",
    "</ol>\n",
    "\n",
    "\n",
    "### SVM Kernels\n",
    "\n",
    "<ol>\n",
    "<li>Linear</li><br>\n",
    "\n",
    "<li>Polynomial</li><br>\n",
    "\n",
    "<li>Radial Basis Function (rbf)</li><br>\n",
    "    \n",
    "<li>Sigmoid</li><br>\n",
    "\n",
    "</ol>\n",
    "\n",
    "\n",
    "1.**Linear**:\n",
    "\n",
    "K(x1,x2) = X1<sup>T</sup>X2\n",
    "\n",
    "\n",
    "2.**Polynomial**:\n",
    "\n",
    "K(x1,x2) = (X1<sup>T</sup>X2 +r)<sup>d</sup>\n",
    "\n",
    "3.**Radial Basis Function (rbf)**:\n",
    "\n",
    "K(x1,x2) = exp(-γ || X1 - X2|| <sup>2</sup>)\n",
    "\n",
    "\n",
    "\n",
    "4.**Sigmoid Kernel**:\n",
    "\n",
    "K(x1,x2) = tanh (γ . X<sub>1</sub><sup>T</sup>X<sub>2</sub>+r)\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "<p style=\"color:red\">Here we are going to use Hinge Loss</p>\n",
    "\n",
    "## <mark>L = max(0,1 - y<sub>i</sub>( W<sup>T</sup>.X + b))</mark>\n",
    "\n",
    "`0 - For Correct`\n",
    "\n",
    "`1 - For Wrong`\n",
    "\n",
    "`loss = max(0, 1 - decision_value)\n",
    "`\n",
    "\n",
    "\n",
    "## Gradient for SVM Classifier\n",
    "\n",
    "### if ( y . (W.X - b)>=1) :\n",
    "\n",
    "dJ/dw = 2 λw\n",
    "\n",
    "dJ/db = 0 \n",
    "\n",
    "### else ( y . (W.X - b)<1):\n",
    "\n",
    "\n",
    "dJ/dw = 2 λw - y.x\n",
    "\n",
    "dJ/db = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Form and the Alpha Coefficients\n",
    "\n",
    "When data is not linearly separable, we switch to a kernelized SVM, which relies on the dual form of the SVM optimization problem.\n",
    "\n",
    "In the dual form:\n",
    "\n",
    "<ol>\n",
    "\n",
    "<li>Instead of learning the weights directly, we learn alpha coefficients (aplha) or each training sample i </li><br>\n",
    "\n",
    "<li>Each alpha<sub>i</sub> represents the \"importance\" of each training sample in determining the decision boundary</li><br>\n",
    "\n",
    "<li>Most of the alpha<sub>i</sub> values will be zero except for a few \"support vectors\" — the points closest to the decision boundary that influence its position and orientation.</li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.01, num_of_iter=1000, lambda_parameter=0.01, kernel='linear'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_of_iter = num_of_iter\n",
    "        self.lambda_parameter = lambda_parameter\n",
    "        self.kernel = kernel\n",
    "        self.alpha = None  # Coefficients for support vectors\n",
    "        self.bias = 0\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    # Kernel functions\n",
    "    def linear_kernel(self, X1, X2):\n",
    "        return cp.dot(X1, X2)\n",
    "\n",
    "    def polynomial_kernel(self, X1, X2, degree=3, r=1):\n",
    "        return (cp.dot(X1, X2) + r) ** degree\n",
    "\n",
    "    def rbf_kernel(self, X1, X2, gamma=0.1):\n",
    "        return cp.exp(-gamma * cp.linalg.norm(X1 - X2) ** 2)\n",
    "\n",
    "    def sigmoid_kernel(self, X1, X2, gamma=0.1, r=1):\n",
    "        return cp.tanh(gamma * cp.dot(X1, X2) + r)\n",
    "\n",
    "    def apply_kernel(self, X1, X2):\n",
    "        if self.kernel == 'linear':\n",
    "            return self.linear_kernel(X1, X2)\n",
    "        elif self.kernel == 'polynomial':\n",
    "            return self.polynomial_kernel(X1, X2)\n",
    "        elif self.kernel == 'rbf':\n",
    "            return self.rbf_kernel(X1, X2)\n",
    "        elif self.kernel == 'sigmoid':\n",
    "            return self.sigmoid_kernel(X1, X2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel: {self.kernel}\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X = cp.asarray(X)\n",
    "        y = cp.asarray(y)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.rows, self.cols = X.shape\n",
    "        self.alpha = cp.zeros(self.rows)  # Alpha coefficients for dual SVM\n",
    "        self.bias = 0\n",
    "        y_mod = cp.where(y == 0, -1, 1)  # Convert labels to -1 and 1\n",
    "        self.X_train = X\n",
    "        self.y_train = y_mod\n",
    "\n",
    "        # Training using stochastic gradient descent on the dual form\n",
    "        for _ in range(self.num_of_iter):\n",
    "            for i in range(self.rows):\n",
    "                # Calculate the decision function with the kernel applied\n",
    "                decision_value = y_mod[i] * (cp.sum(cp.array([self.alpha[j] * y_mod[j] * self.apply_kernel(X[j], X[i])\n",
    "                                                     for j in range(self.rows)])) + self.bias)\n",
    "                \n",
    "                # Hinge loss condition\n",
    "                if decision_value < 1:\n",
    "                    # Update alpha and bias using hinge loss gradient\n",
    "                    self.alpha[i] += self.learning_rate * (1 - decision_value)\n",
    "                    self.bias += self.learning_rate * y_mod[i]\n",
    "                else:\n",
    "                    # L2 regularization only\n",
    "                    self.alpha[i] -= self.learning_rate * self.lambda_parameter * self.alpha[i]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = cp.asarray(X)  \n",
    "        y_pred = []\n",
    "        \n",
    "        for x in X:\n",
    "            # Sum up contributions from each support vector\n",
    "            output = cp.sum(cp.array([self.alpha[j] * self.y_train[j] * self.apply_kernel(self.X_train[j], x)\n",
    "                             for j in range(self.rows)])) + self.bias\n",
    "            \n",
    "            predicted_label = cp.sign(output)\n",
    "            y_pred.append(1 if predicted_label > 0 else 0)\n",
    "        \n",
    "        return cp.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"diabetes_prediction_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['gender'] == 'Other', 'gender'] = 'Female'\n",
    "\n",
    "df['gender'] = pd.get_dummies(df['gender'],drop_first=True).astype(int)\n",
    "\n",
    "df.drop(columns='smoking_history',inplace=bool(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X , x ,Y , y = train_test_split(df.iloc[:,:-1].values,df.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuraccy of the model is \",accuracy_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
